{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Evaluate different ways to export a torch model to ONNX\n\nThe example evaluates the performance of onnxruntime of a simple\ntorch model after it was converted into ONNX through different processes:\n\n* [TorchScript-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter),\n  let's call it **script**\n* [TorchDynamo-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter),\n  let's call it **dynamo**\n* if available, the previous model but optimized, **dynopt**\n* a custom exporter **cus_p0**, this exporter supports a very limited\n  set of models, as **dynamo**, it relies on\n  [torch.fx](https://pytorch.org/docs/stable/fx.html) but the design is closer to\n  what tensorflow-onnx does.\n* the same exporter but unused nodes were removed and constants were folded, **cus_p2**\n\nTo run the script:\n\n::\n\n    python docs/examples/plot_torch_export --help\n\nThe script takes around 12 minutes with a larger models.\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport itertools\nimport os\nimport platform\nimport pprint\nimport multiprocessing\nimport time\nimport cProfile\nimport pstats\nimport io\nimport warnings\nimport logging\nfrom pstats import SortKey\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nimport onnx\nfrom onnxrt_backend_dev.monitoring.profiling import profile2graph\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport onnxrt_backend_dev\n\n# from onnxrt_backend_dev.torch_exp.onnx_export import to_onnx\nfrom onnxrt_backend_dev.plotting.memory import memory_peak_plot\nfrom onnxrt_backend_dev.ext_test_case import get_figure\nfrom onnxrt_backend_dev.args import get_parsed_args\nfrom onnxrt_backend_dev.monitoring.benchmark import measure_time\nfrom onnxrt_backend_dev.monitoring.memory_peak import start_spying_on\nfrom tqdm import tqdm\n\nhas_cuda = has_cuda and torch.cuda.is_available()\nlogging.disable(logging.ERROR)\n\n\ndef system_info():\n    obs = {}\n    obs[\"processor\"] = platform.processor()\n    obs[\"cores\"] = multiprocessing.cpu_count()\n    try:\n        obs[\"cuda\"] = 1 if torch.cuda.is_available() else 0\n        obs[\"cuda_count\"] = torch.cuda.device_count()\n        obs[\"cuda_name\"] = torch.cuda.get_device_name()\n        obs[\"cuda_capa\"] = torch.cuda.get_device_capability()\n    except (RuntimeError, AssertionError):\n        # no cuda\n        pass\n    return obs\n\n\npprint.pprint(system_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scripts arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "script_args = get_parsed_args(\n    \"plot_torch_export\",\n    description=__doc__,\n    scenarios={\n        \"small\": \"small model to test\",\n        \"middle\": \"55Mb model\",\n        \"large\": \"1Gb model\",\n    },\n    warmup=5,\n    repeat=5,\n    maxtime=(\n        2,\n        \"maximum time to run a model to measure the computation time, \"\n        \"it is 0.1 when scenario is small\",\n    ),\n    expose=\"scenarios,repeat,warmup\",\n)\n\nif script_args.scenario in (None, \"small\"):\n    script_args.maxtime = 0.1\nprint(f\"scenario={script_args.scenario or 'small'}\")\nprint(f\"warmup={script_args.warmup}\")\nprint(f\"repeat={script_args.repeat}\")\nprint(f\"maxtime={script_args.maxtime}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The model\n\nA simple model to convert.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyModelClass(nn.Module):\n    def __init__(self, scenario=script_args.scenario):\n        super(MyModelClass, self).__init__()\n        if scenario == \"middle\":\n            self.large = False\n            self.conv1 = nn.Conv2d(1, 128, 5)\n            self.conv2 = nn.Conv2d(128, 16, 5)\n            self.fc1 = nn.Linear(13456, 1024)\n            self.fcs = []\n            self.fc2 = nn.Linear(1024, 128)\n            self.fc3 = nn.Linear(128, 10)\n        elif scenario in (None, \"small\"):\n            self.large = False\n            self.conv1 = nn.Conv2d(1, 16, 5)\n            self.conv2 = nn.Conv2d(16, 16, 5)\n            self.fc1 = nn.Linear(16, 512)\n            self.fcs = []\n            self.fc2 = nn.Linear(512, 128)\n            self.fc3 = nn.Linear(128, 10)\n        elif scenario in (None, \"large\"):\n            self.large = True\n            self.conv1 = nn.Conv2d(1, 128, 5)\n            self.conv2 = nn.Conv2d(128, 16, 5)\n            self.fc1 = nn.Linear(13456, 4096)\n            # torch script does not support loops.\n            self.fca = nn.Linear(4096, 4096)\n            self.fcb = nn.Linear(4096, 4096)\n            self.fcc = nn.Linear(4096, 4096)\n            self.fcd = nn.Linear(4096, 4096)\n            self.fce = nn.Linear(4096, 4096)\n            self.fcf = nn.Linear(4096, 4096)\n            self.fcg = nn.Linear(4096, 4096)\n            self.fch = nn.Linear(4096, 4096)\n            self.fci = nn.Linear(4096, 4096)\n            self.fck = nn.Linear(4096, 4096)\n            self.fcl = nn.Linear(4096, 4096)\n            self.fcm = nn.Linear(4096, 4096)\n            self.fcn = nn.Linear(4096, 4096)\n            # end of the unfolded loop.\n            self.fc2 = nn.Linear(4096, 128)\n            self.fc3 = nn.Linear(128, 10)\n        else:\n            raise ValueError(f\"Unsupported scenario={scenario!r}.\")\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        if self.large:\n            # loop\n            x = F.relu(self.fca(x))\n            x = F.relu(self.fcb(x))\n            x = F.relu(self.fcc(x))\n            x = F.relu(self.fcd(x))\n            x = F.relu(self.fce(x))\n            x = F.relu(self.fcf(x))\n            x = F.relu(self.fcg(x))\n            x = F.relu(self.fch(x))\n            x = F.relu(self.fci(x))\n            x = F.relu(self.fck(x))\n            x = F.relu(self.fcl(x))\n            x = F.relu(self.fcm(x))\n            x = F.relu(self.fcn(x))\n            # end of the loop\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef create_model_and_input(scenario=script_args.scenario):\n    if scenario == \"middle\":\n        shape = [1, 1, 128, 128]\n    elif scenario in (None, \"small\"):\n        shape = [1, 1, 16, 16]\n    elif scenario == \"large\":\n        shape = [1, 1, 128, 128]\n    else:\n        raise ValueError(f\"Unsupported scenario={scenario!r}.\")\n    input_tensor = torch.rand(*shape).to(torch.float32)\n    model = MyModelClass(scenario=scenario)\n    assert model(input_tensor) is not None\n    return model, input_tensor\n\n\ndef torch_model_size(model):\n    size_model = 0\n    for param in model.parameters():\n        size = param.numel() * torch.finfo(param.data.dtype).bits / 8\n        size_model += size\n    return size_model\n\n\nmodel, input_tensor = create_model_and_input()\nmodel_size = torch_model_size(model)\nprint(f\"model size={model_size / 2 ** 20} Mb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def export_script(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            torch.onnx.export(model, *args, filename, input_names=[\"input\"])\n\n\ndef export_dynamo(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            export_output = torch.onnx.dynamo_export(model, *args)\n            export_output.save(filename)\n\n\ndef export_dynopt(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            export_output = torch.onnx.dynamo_export(model, *args)\n            model_onnx = export_output.model_proto\n\n            from onnxrewriter.optimizer import optimize\n\n            optimized_model = optimize(model_onnx)\n            with open(filename, \"wb\") as f:\n                f.write(optimized_model.SerializeToString())\n\n\ndef export_cus_p0(filename, model, *args):\n    onx = to_onnx(model, tuple(args), input_names=[\"input\"])  # noqa: F821\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\n\n\ndef export_cus_p2(filename, model, *args):\n    onx = to_onnx(  # noqa: F821\n        model,\n        tuple(args),\n        input_names=[\"input\"],\n        remove_unused=True,\n        constant_folding=True,\n    )\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check they are working.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "export_functions = [\n    export_script,\n    export_dynamo,\n    export_dynopt,\n    export_cus_p0,\n    export_cus_p2,\n]\n\nexporters = {f.__name__.replace(\"export_\", \"\"): f for f in export_functions}\n\nsupported_exporters = {}\nfor k, v in exporters.items():\n    print(f\"run exporter {k}\")\n    filename = f\"plot_torch_export_{k}.onnx\"\n    try:\n        v(filename, model, input_tensor)\n    except Exception as e:\n        print(f\"skipped due to {str(e)[:1000]}\")\n        continue\n    supported_exporters[k] = v\n    print(f\"done. size={os.stat(filename).st_size / 2 ** 20:1.0f} Mb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporter memory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten(ps):\n    obs = ps[\"cpu\"].to_dict(unit=2**20)\n    if \"gpus\" in ps:\n        for i, g in enumerate(ps[\"gpus\"]):\n            for k, v in g.to_dict(unit=2**20).items():\n                obs[f\"gpu{i}_{k}\"] = v\n    return obs\n\n\ndata = []\n\nfor k, v in supported_exporters.items():\n    print(f\"run exporter for memory {k}\")\n    filename = f\"plot_torch_export_{k}.onnx\"\n    if has_cuda:\n        torch.cuda.set_device(0)\n    stat = start_spying_on(cuda=1 if has_cuda else 0)\n    v(filename, model, input_tensor)\n    obs = flatten(stat.stop())\n    print(\"done.\")\n    onx = onnx.load(filename)\n    obs.update(dict(nodes=len(onx.graph.node), export=k))\n    data.append(obs)\n\nstat = start_spying_on(cuda=1 if has_cuda else 0)\nexported_mod = torch.export.export(model, (input_tensor,))\nobs = flatten(stat.stop())\nobs.update(dict(export=\"torch.fx\"))\ndata.append(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df1 = pandas.DataFrame(data)\ndf1.to_csv(\"plot_torch_export_memory.csv\", index=False)\ndf1.to_excel(\"plot_torch_export_memory.xlsx\", index=False)\nprint(df1)\n\nax = memory_peak_plot(\n    data,\n    bars=[model_size * i / 2**20 for i in range(1, 5)],\n    suptitle=f\"Memory Consumption of the Export\\n\"\n    f\"model size={model_size / 2**20:1.0f} Mb\",\n)\nget_figure(ax).savefig(\"plot_torch_export_memory.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporter speed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\n\nfor k, v in supported_exporters.items():\n    print(f\"run exporter {k}\")\n    filename = f\"plot_torch_export_{k}.onnx\"\n    times = []\n    for i in range(script_args.repeat):\n        begin = time.perf_counter()\n        v(filename, model, input_tensor)\n        duration = time.perf_counter() - begin\n        times.append(duration)\n    onx = onnx.load(filename)\n    print(\"done.\")\n    data.append(\n        dict(\n            export=k,\n            time=np.mean(times),\n            min=min(times),\n            max=max(times),\n            first=times[0],\n            last=times[-1],\n            std=np.std(times),\n            nodes=len(onx.graph.node),\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last export to measure time torch spends in export the model\nbefore any other export can begin the translation\nexcept the first one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = []\nfor i in range(script_args.repeat):\n    begin = time.perf_counter()\n    exported_mod = torch.export.export(model, (input_tensor,))\n    duration = time.perf_counter() - begin\n    times.append(duration)\ndata.append(\n    dict(\n        export=\"torch.fx\",\n        time=np.mean(times),\n        min=min(times),\n        max=max(times),\n        first=times[0],\n        last=times[-1],\n        std=np.std(times),\n        nodes=len(onx.graph.node),\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df1 = pandas.DataFrame(data)\ndf1.to_csv(\"plot_torch_export_time.csv\", index=False)\ndf1.to_excel(\"plot_torch_export_time.xlsx\", index=False)\nprint(df1)\n\nfig, ax = plt.subplots(1, 1)\ndfi = df1[[\"export\", \"time\", \"std\"]].set_index(\"export\")\ndfi[\"time\"].plot.bar(ax=ax, title=\"Export time\", yerr=dfi[\"std\"], rot=30)\nfig.tight_layout()\nfig.savefig(\"plot_torch_export_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporter Profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n    pathes = [\n        os.path.abspath(\n            os.path.normpath(os.path.join(os.path.dirname(torch.__file__), \"..\"))\n        ),\n        os.path.abspath(\n            os.path.normpath(os.path.join(os.path.dirname(onnx.__file__), \"..\"))\n        ),\n        os.path.abspath(\n            os.path.normpath(\n                os.path.join(os.path.dirname(onnxrt_backend_dev.__file__), \"..\")\n            )\n        ),\n    ]\n    for p in pathes:\n        text = text.replace(p, \"\")\n    text = text.replace(\"onnxrt_backend_dev\", \"onnxrt_backend_dev\".upper())\n    return text\n\n\ndef profile_function(name, export_function, verbose=False):\n    print(f\"profile {name}: {export_function}\")\n    pr = cProfile.Profile()\n    pr.enable()\n    for i in range(script_args.repeat):\n        export_function(\"dummyc.onnx\", model, input_tensor)\n    pr.disable()\n    s = io.StringIO()\n    sortby = SortKey.CUMULATIVE\n    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n    ps.print_stats()\n\n    raw = s.getvalue()\n    text = \"\\n\".join(raw.split(\"\\n\")[:200])\n    if verbose:\n        print(text)\n    with open(f\"plot_torch_export_profile_{name}.txt\", \"w\") as f:\n        f.write(raw)\n\n    root, nodes = profile2graph(ps, clean_text=clean_text)\n    text = root.to_text()\n    with open(f\"plot_torch_export_profile_{name}_h.txt\", \"w\") as f:\n        f.write(text)\n    print(\"done.\")\n\n\n# profile_function(\"custom0\", export_cus_p0, True)\n# profile_function(\"custom2\", export_cus_p2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same with dynamo-exporter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "profile_function(\"dynamo\", export_dynamo, verbose=True)\nif \"dynopt\" in supported_exporters:\n    profile_function(\"dynopt\", export_dynopt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark exported models with ORT\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(shape):\n    from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\n\n    data = []\n    data1 = []\n    data_mem_load = []\n    data_mem_first_run = []\n    data_mem_run = []\n    confs = list(\n        itertools.product(\n            [_ for _ in os.listdir(\".\") if \".onnx\" in _ and _.startswith(\"plot_torch\")],\n            [\n                [\"CPUExecutionProvider\"],\n                [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n            ],\n            [\"0\", \"1\"],\n        )\n    )\n    loop = tqdm(confs)\n    print(f\"number of experiments: {len(loop)}\")\n    for name, ps, aot in loop:\n        root = os.path.split(name)[-1]\n        _, ext = os.path.splitext(root)\n        if ext != \".onnx\":\n            continue\n\n        obs = {}  # system_info()\n        obs[\"name\"] = name\n        obs[\"providers\"] = \",\".join(ps)\n        p = \"CUDA\" if \"CUDA\" in obs[\"providers\"] else \"CPU\"\n        obs[\"compute\"] = p\n        obs[\"aot\"] = 1 if aot == \"0\" else 0\n        obs[\"export\"] = name.replace(\"plot_torch_export_\", \"\").replace(\".onnx\", \"\")\n\n        onx = onnx.load(name)\n        obs[\"n_nodes\"] = len(onx.graph.node)\n        obs[\"n_function\"] = len(onx.functions or [])\n        obs[\"n_sub\"] = len([n for n in onx.graph.node if n.op_type == \"Sub\"])\n        obs1 = obs.copy()\n        short_obs = dict(\n            name=obs[\"name\"],\n            aot=obs[\"aot\"],\n            providers=obs[\"providers\"],\n            export=obs[\"export\"],\n            compute=obs[\"compute\"],\n        )\n\n        opts = SessionOptions()\n        opts.add_session_config_entry(\"session.disable_aot_function_inlining\", aot)\n        opts.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n        opts.optimized_model_filepath = (\n            f\"ort-{name.replace('.onnx', '')}-{p.lower()}-\"\n            f\"aot{1 if aot == '0' else 0}.onnx\"\n        )\n\n        try:\n            InferenceSession(name, opts, providers=ps)\n        except Exception as e:\n            loop.set_description(f\"ERROR-load: {name} {e}\")\n            obs.update({\"error\": e, \"step\": \"run\"})\n            data.append(obs)\n            continue\n\n        opts = SessionOptions()\n        opts.add_session_config_entry(\"session.disable_aot_function_inlining\", aot)\n        opts.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n        stat = start_spying_on(cuda=1 if has_cuda else 0)\n        sess = InferenceSession(name, opts, providers=ps)\n        memobs = flatten(stat.stop())\n        memobs.update(short_obs)\n        data_mem_load.append(memobs)\n\n        input_name = sess.get_inputs()[0].name\n        feeds = {input_name: np.random.rand(*shape).astype(np.float32)}\n\n        stat = start_spying_on(cuda=1 if has_cuda else 0)\n        try:\n            sess.run(None, feeds)\n        except Exception as e:\n            loop.set_description(f\"ERROR-run: {name} {e}\")\n            obs.update({\"error\": e, \"step\": \"load\"})\n            data.append(obs)\n            stat.stop()\n            continue\n        memobs = flatten(stat.stop())\n        memobs.update(short_obs)\n        data_mem_first_run.append(memobs)\n\n        # memory consumption\n        stat = start_spying_on(cuda=1 if has_cuda else 0)\n        for i in range(0, script_args.warmup):\n            sess.run(None, feeds)\n        memobs = flatten(stat.stop())\n        memobs.update(short_obs)\n        data_mem_run.append(memobs)\n\n        obs.update(\n            measure_time(\n                lambda: sess.run(None, feeds),\n                max_time=script_args.maxtime,\n                repeat=script_args.repeat,\n                number=1,\n            )\n        )\n\n        loop.set_description(f\"{obs['average']} {name} {ps}\")\n        data.append(obs)\n\n        # check first run\n        obs1.update(\n            measure_time(\n                lambda: InferenceSession(name, opts, providers=ps).run(None, feeds),\n                max_time=script_args.maxtime,\n                repeat=max(1, script_args.repeat // 2),\n                number=1,\n            )\n        )\n        data1.append(obs1)\n\n    df = pandas.DataFrame(data)\n    df.to_csv(\"plot_torch_export_ort_time.csv\", index=False)\n    df.to_excel(\"plot_torch_export_ort_time.xlsx\", index=False)\n    df1 = pandas.DataFrame(data1)\n    df1.to_csv(\"plot_torch_export_ort_time1_init.csv\", index=False)\n    df1.to_excel(\"plot_torch_export_ort_time1_init.xlsx\", index=False)\n    dfmem = pandas.DataFrame(data_mem_load)\n    dfmem.to_csv(\"plot_torch_export_ort_load_mem.csv\", index=False)\n    dfmem.to_excel(\"plot_torch_export_ort_load_mem.xlsx\", index=False)\n    dfmemr = pandas.DataFrame(data_mem_run)\n    dfmemr.to_csv(\"plot_torch_export_ort_run_mem.csv\", index=False)\n    dfmemr.to_excel(\"plot_torch_export_ort_run_mem.xlsx\", index=False)\n    dfmemfr = pandas.DataFrame(data_mem_first_run)\n    dfmemfr.to_csv(\"plot_torch_export_ort_first_run_mem.csv\", index=False)\n    dfmemfr.to_excel(\"plot_torch_export_ort_first_run_mem.xlsx\", index=False)\n    return df, df1, dfmem, dfmemfr, dfmemr\n\n\ndf, df_init, dfmem, dfmemfr, dfmemr = benchmark(list(input_tensor.shape))\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other view\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def view_time(df, title, suffix=\"time\"):\n    piv = pandas.pivot_table(\n        df, index=\"export\", columns=[\"compute\", \"aot\"], values=\"average\"\n    )\n    print(piv)\n    piv.to_csv(f\"plot_torch_export_ort_{suffix}_compute.csv\")\n    piv.to_excel(f\"plot_torch_export_ort_{suffix}_compute.xlsx\")\n\n    piv_gpu = pandas.pivot_table(\n        df[df.compute == \"CUDA\"],\n        index=\"export\",\n        columns=[\"compute\", \"aot\"],\n        values=\"average\",\n    )\n    piv_cpu = pandas.pivot_table(\n        df[df.compute == \"CPU\"],\n        index=\"export\",\n        columns=[\"compute\", \"aot\"],\n        values=\"average\",\n    )\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    fig.suptitle(title)\n    piv_cpu.plot.barh(ax=ax[0], title=\"CPU\")\n    piv_gpu.plot.barh(ax=ax[1], title=\"CUDA\")\n    fig.tight_layout()\n    fig.savefig(f\"plot_torch_export_ort_{suffix}.png\")\n    return ax\n\n\nview_time(df, \"Compares onnxruntime time on exported models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New graph without the very long times.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv_cpu = pandas.pivot_table(\n    df[\n        (df.compute == \"CPU\")\n        & ((df.aot == 1) | ((df.export != \"dynamo\") & (df.export != \"dynopt\")))\n    ],\n    index=\"export\",\n    columns=[\"compute\", \"aot\"],\n    values=\"average\",\n)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nfig.suptitle(\"Compares onnxruntime time on exported models\\nHide dynamo without AOT\")\npiv_cpu.plot.barh(ax=ax[0], title=\"CPU\")\nif has_cuda:\n    piv_gpu = pandas.pivot_table(\n        df[df.compute == \"CUDA\"],\n        index=\"export\",\n        columns=[\"compute\", \"aot\"],\n        values=\"average\",\n    )\n    piv_gpu.plot.barh(ax=ax[1], title=\"CUDA\")\nfig.tight_layout()\nfig.savefig(\"plot_torch_export_ort_time_2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do the same with the loading time + the first run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "view_time(\n    df_init,\n    \"Compares onnxruntime loading time and first run on exported models\",\n    suffix=\"time1_init\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Loading Time (ORT)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for compute in [\"CPU\", \"CUDA\"]:\n    ax = memory_peak_plot(\n        dfmem[dfmem.compute == compute],\n        (\"export\", \"aot\"),\n        suptitle=f\"Memory Consumption of onnxruntime loading time\"\n        f\"\\nrunning on {compute}\",\n        bars=[model_size * i / 2**20 for i in range(1, 3)],\n        figsize=(18, 6),\n    )\n    get_figure(ax).savefig(f\"plot_torch_export_ort_load_mem_{compute}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory First Running Time (ORT)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for compute in [\"CPU\", \"CUDA\"]:\n    ax = memory_peak_plot(\n        dfmemfr[dfmemfr.compute == compute],\n        (\"export\", \"aot\"),\n        suptitle=f\"Memory Consumption of onnxruntime first running time\"\n        f\"\\nrunning on {compute}\",\n        bars=[model_size * i / 2**20 for i in range(1, 3)],\n        figsize=(18, 6),\n    )\n    get_figure(ax).savefig(f\"plot_torch_export_ort_first_run_mem_{compute}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Running Time (ORT)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for compute in [\"CPU\", \"CUDA\"]:\n    ax = memory_peak_plot(\n        dfmemr[dfmemr.compute == compute],\n        (\"export\", \"aot\"),\n        suptitle=f\"Memory Consumption of onnxruntime running time\"\n        f\"\\nrunning on {compute}\",\n        bars=[model_size * i / 2**20 for i in range(1, 3)],\n        figsize=(18, 6),\n    )\n    get_figure(ax).savefig(f\"plot_torch_export_ort_run_mem_{compute}.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}